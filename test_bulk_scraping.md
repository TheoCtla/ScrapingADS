# Guide de Test - Fonctionnalit√© de Scraping en Masse

## Pr√©requis

1. **Serveurs d√©marr√©s** :
   - Frontend : `npm run dev` (port 5173)
   - Backend : `python run.py` (port 5050)

2. **Configuration** :
   - Clients configur√©s dans `backend/config/client_allowlist.json`
   - Mappings Google/Meta configur√©s

## Tests √† effectuer

### 1. Test de base - Interface utilisateur

#### ‚úÖ V√©rifications visuelles
- [ ] Le bouton "Scraper tous les clients" appara√Æt √† c√¥t√© du bouton d'export unitaire
- [ ] Le bouton est vert et styl√© diff√©remment du bouton principal
- [ ] Le bouton est d√©sactiv√© si aucune m√©trique n'est s√©lectionn√©e
- [ ] Le bouton est d√©sactiv√© pendant le scraping en cours

#### ‚úÖ Comportement du bouton
- [ ] Le bouton affiche "Scraper tous les clients" par d√©faut
- [ ] Le bouton affiche "Scraping en cours..." pendant le traitement
- [ ] Le bouton redevient actif apr√®s la fin du traitement

### 2. Test de progression

#### ‚úÖ Composant de progression
- [ ] Le composant `BulkScrapingProgress` appara√Æt pendant le traitement
- [ ] La barre de progression se remplit progressivement
- [ ] Les compteurs (i/N, succ√®s, √©checs) s'actualisent en temps r√©el
- [ ] Le nom du client en cours s'affiche correctement

#### ‚úÖ Statistiques
- [ ] Progression : "2 / 28" (exemple)
- [ ] Succ√®s : nombre de clients trait√©s avec succ√®s
- [ ] √âchecs : nombre de clients en √©chec

### 3. Test de traitement s√©quentiel

#### ‚úÖ Logs console
Ouvrir la console du navigateur et v√©rifier les logs :
```
üì∏ Contexte UI captur√©: {startDate, endDate, metrics...}
üîÑ Traitement 1/28: Client A
üì§ Envoi du payload pour Client A
‚úÖ Succ√®s pour Client A
üîÑ Traitement 2/28: Client B
...
```

#### ‚úÖ S√©quentialit√©
- [ ] Les clients sont trait√©s un par un (pas de parall√©lisme)
- [ ] Pause de 1s entre chaque client
- [ ] Retry automatique en cas d'√©chec (max 2 tentatives)

### 4. Test de gestion d'erreurs

#### ‚úÖ Retry automatique
- [ ] En cas d'erreur, retry automatique apr√®s 2s
- [ ] Deuxi√®me retry apr√®s 4s si n√©cessaire
- [ ] √âchec d√©finitif apr√®s 3 tentatives

#### ‚úÖ Continuation du traitement
- [ ] Le traitement continue m√™me si un client √©choue
- [ ] Les √©checs sont enregistr√©s et affich√©s
- [ ] Le r√©sum√© final inclut les succ√®s et √©checs

### 5. Test d'annulation

#### ‚úÖ Bouton d'annulation
- [ ] Le bouton "Annuler" est actif pendant le traitement
- [ ] Le bouton est d√©sactiv√© une fois termin√©
- [ ] Cliquer sur "Annuler" arr√™te le traitement apr√®s l'√©tape en cours

#### ‚úÖ Arr√™t propre
- [ ] Le traitement s'arr√™te proprement
- [ ] L'√©tat est nettoy√© correctement
- [ ] Les r√©sultats partiels sont conserv√©s

### 6. Test de contexte UI

#### ‚úÖ Capture du contexte
- [ ] Les s√©lections UI sont captur√©es au d√©marrage
- [ ] Le m√™me contexte est appliqu√© √† tous les clients
- [ ] Les modifications UI pendant le traitement n'affectent pas le run en cours

#### ‚úÖ Param√®tres respect√©s
- [ ] Plage de dates s√©lectionn√©e
- [ ] M√©triques Google/Meta s√©lectionn√©es
- [ ] Options de scraping (Contact, Itin√©raires)
- [ ] Mois du Google Sheet

### 7. Test de r√©sum√© final

#### ‚úÖ Alert de fin
- [ ] Alert avec nombre de succ√®s et √©checs
- [ ] Format : "Scraping termin√©! ‚úÖ Succ√®s: X ‚ùå √âchecs: Y"

#### ‚úÖ D√©tail des √©checs
- [ ] Liste des clients en √©chec avec erreurs
- [ ] Affichage dans le composant de progression
- [ ] Persistance jusqu'au rechargement de la page

### 8. Test d'int√©gration avec le backend

#### ‚úÖ Endpoints
- [ ] `/list-authorized-clients` : Liste compl√®te des clients
- [ ] `/list-filtered-clients` : Clients filtr√©s (simulation searchbar)
- [ ] `/export-unified-report` : Traitement de chaque client

#### ‚úÖ R√©utilisation des services
- [ ] M√™me logique de r√©solution des clients
- [ ] M√™me logique de scraping Google/Meta
- [ ] M√™me logique d'export vers Google Sheets

## Sc√©narios de test

### Sc√©nario 1 : Traitement complet
1. S√©lectionner des m√©triques Google et Meta
2. Cliquer sur "Scraper tous les clients"
3. Observer la progression
4. V√©rifier le r√©sum√© final

### Sc√©nario 2 : Gestion d'erreurs
1. Simuler une erreur r√©seau (d√©connecter temporairement)
2. Lancer le scraping
3. V√©rifier les retries et la gestion d'erreurs
4. Reconnecter et v√©rifier la continuation

### Sc√©nario 3 : Annulation
1. Lancer le scraping
2. Cliquer sur "Annuler" au milieu du traitement
3. V√©rifier l'arr√™t propre
4. V√©rifier les r√©sultats partiels

### Sc√©nario 4 : Contexte UI
1. S√©lectionner des param√®tres sp√©cifiques
2. Lancer le scraping
3. Modifier les param√®tres pendant le traitement
4. V√©rifier que les anciens param√®tres sont utilis√©s

## Validation des crit√®res d'acceptation

### ‚úÖ Crit√®res valid√©s
- [ ] Tous les clients de la searchbar sont trait√©s un par un
- [ ] Les m√™mes s√©lections UI sont appliqu√©es √† chaque client
- [ ] Les m√™mes fonctions d'export au Google Sheet sont utilis√©es
- [ ] Annulation possible avec arr√™t propre
- [ ] Logs clairs par client (d√©but/fin, plateformes, dur√©e, retries, erreurs)
- [ ] Aucun client en dehors de la searchbar n'est appel√©

### ‚úÖ Fonctionnalit√©s bonus
- [ ] Barre de progression visuelle
- [ ] Compteurs en temps r√©el
- [ ] R√©sum√© d√©taill√© des √©checs
- [ ] Retry automatique avec backoff
- [ ] Interface responsive

## D√©pannage

### Probl√®mes courants

#### Le bouton n'appara√Æt pas
- V√©rifier que `authorizedClients.length > 0`
- V√©rifier la console pour les erreurs de chargement

#### Le traitement ne d√©marre pas
- V√©rifier qu'au moins une plateforme est s√©lectionn√©e
- V√©rifier les logs console pour les erreurs

#### Erreurs de r√©seau
- V√©rifier que le backend est accessible sur le port 5050
- V√©rifier les CORS dans la configuration

#### Progression bloqu√©e
- V√©rifier les logs console pour les erreurs d√©taill√©es
- V√©rifier les timeouts et retries

## M√©triques de performance

### Temps de traitement
- Temps moyen par client : ~5-10s
- Pause entre clients : 1s
- Retry delay : 2s, 4s

### Utilisation des ressources
- CPU : Faible (traitement s√©quentiel)
- M√©moire : Stable (pas d'accumulation)
- R√©seau : Mod√©r√© (1 requ√™te par client)

## Conclusion

La fonctionnalit√© de scraping en masse est maintenant op√©rationnelle avec :
- ‚úÖ Interface utilisateur compl√®te
- ‚úÖ Traitement s√©quentiel robuste
- ‚úÖ Gestion d'erreurs avanc√©e
- ‚úÖ R√©utilisation des services existants
- ‚úÖ Documentation compl√®te
- ‚úÖ Tests unitaires
